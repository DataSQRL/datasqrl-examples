# DataSQRL Examples

## Getting Started Examples

* **[Getting Started Guide](getting-started-examples/00_getting_started/)**: Introduction and overview of DataSQRL with foundational concepts for building data pipelines.
* **[Kafka to Console](getting-started-examples/01_kafka_to_console/)**: Simple pipeline that reads data from a Kafka topic and outputs to console for basic testing.
* **[Kafka to Kafka](getting-started-examples/02_kafka_to_kafka/)**: Reads from one Kafka topic, transforms the data, and writes to another Kafka topic.
* **[Two Streams Join](getting-started-examples/03_two_streams_kafka_to_kafka/)**: Combines data from two Kafka topics using temporal joins and writes enriched output to Kafka.
* **[External Kafka Streams](getting-started-examples/04_two_streams_external_kafka_to_kafka/)**: Multi-stream joins using external Kafka setup for decoupled integration scenarios.
* **[File to Iceberg](getting-started-examples/05_file_iceberg_test/)**: Reads data from local files and writes to Iceberg tables for analytics without cloud dependencies.
* **[Kafka to Iceberg](getting-started-examples/06_external_kafka_iceberg_test/)**: Streams data from Kafka to Iceberg using local warehouse directory for staging and testing.
* **[Kafka to Iceberg with Glue](getting-started-examples/07_external_kafka_iceberg_glue_test/)**: Production-ready pipeline using AWS Glue catalog and S3 storage for Iceberg tables.
* **[Schema Registry Integration](getting-started-examples/08_schema_registry_kafka_to_kafka/)**: Kafka-to-Kafka pipeline using Confluent Schema Registry for Avro schema management.


## Real-World Use Cases and Examples

The following lists real-world use case implementations. These examples are more comprehensive and cover everything you need in production-ready data pipelines and data APIs. They demonstrate how to add integration tests, reuse data sources across multiple pipelines implemented in a shared repository, and many other advanced features of DataSQRL.

* **[Finance Credit Card Chatbot](finance-credit-card-chatbot/)**: Build a data pipeline that enriches and analyzes credit card transaction in real time and feeds the data into a GenAI chatbot to answer customer's questions about their transactions and spending. The extended example shows how to build a credit card rewards program and GenAI agent that sells credit cards.
* **[Clickstream AI Recommendation](clickstream-ai-recommendation/)**: Build a personalized recommendation engine based on clickstream data and vector content embeddings generated by an LLM.
* **[Healthcare Study](healthcare-study-monitoring/)**: Build a data pipeline for enriching healthcare data and querying it in realtime through an API, for data analytics in Iceberg, and publishing it to Kafka.
* **[Law Enforcement](law-enforcement)**: Build a realtime data pipeline for capturing and tracking warrants and Bolos.
* **[Oil & Gas IoT Automation Agent](oil-gas-agent-automation)**: Build a realtime data enrichment pipeline that triggers an agent to analyze abnormal events for automated troubleshooting.
* **[IoT Sensor Metrics](iot-sensor-metrics/)**: Build an event-driven microservice that ingests sensor metrics, processes them in realtime, and produces alerts and dashboards for users.
* **[Logistics Shipping](logistics-shipping-geodata/)**: Build a data pipeline that processes logistics data to provide real-time tracking and shipment information for customers.
* **[Usage Analytics](usage-analytics)**: Build realtime usage analytics for users and customers exposed via API.

## Running the Examples

### Prerequisites

To compile and run these examples with DataSQRL you need to have a [recent version of Docker](https://docs.docker.com/get-docker/) installed on your machine.

### Running & Compiling Examples

To run the examples on Linux or MacOS, open a terminal and run the following command:
```bash
docker run -it --rm -p 8888:8888 -p 8081:8081 -p 9092:9092 -v $PWD:/build datasqrl/cmd:latest run [ARGUMENTS GO HERE]
```

If you are on windows using Powershell, you need to reference the local directory with a slightly different syntax:
```bash
docker run -it --rm -p 8888:8888 -p 8081:8081 -p 9092:9092 -v $PWD:/build datasqrl/cmd:latest run [ARGUMENTS GO HERE]
```

Check the `README.md` in the respective directory for more information on the arguments to run each example. 
We will be using the Unix syntax, so keep in mind that you have to adjust the commands slightly on Windows machines by using `${PWD}` instead.

To compile an example (without running it), use this command:
```bash
docker run -it --rm -v $PWD:/build datasqrl/cmd:latest compile [ARGUMENTS GO HERE]
```

## What is DataSQRL?

Check out the main [DataSQRL repository](https://github.com/DataSQRL/sqrl/) for more information on the compiler and runtime used in these examples.

Take a look at the [DataSQRL documentation](https://datasqrl.github.io/sqrl) to learn how to build your own project with DataSQRL.

## Claude Code

This repository contains a [CLAUDE.md](CLAUDE.md) file that provides context for [Claude Code Agent](https://www.anthropic.com/claude-code) which can assist you in building DataSQRL projects and help you learn how to use DataSQRL.

Clone the repository, start Claude Code in the root folder, and code away.

