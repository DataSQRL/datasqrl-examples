# This is a docker-compose template for starting a DataSQRL compiled data pipeline
# This template uses the Apache Flink as the stream engine, Postgres as the database engine, and Vertx as the server engine.
# It assumes that:
# 1. You ran the `compile` command to compile your SQRL script (and API specification)
# 2. Are in the `build/deploy` directory which contains the deployment artifacts generated by the compiler
# 3. Have built the deployment artifacts for each pipeline engine
# Refer to the deployment documentation for more information:
# https://www.datasqrl.com/docs/reference/operations/deploy/overview/
version: "3.8"
services:
  database:
    image: ankane/pgvector:v0.5.0
    restart: always
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=datasqrl
    ports:
      - '5432:5432'
    volumes:
      - ./database-schema.sql:/docker-entrypoint-initdb.d/init-schema.sql

  flink-jobmanager:
    image: flink:1.16.1-scala_2.12-java11
    ports:
      - "8081:8081"
    command: /bin/bash /exec/init-flink.sh jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
    volumes:
      - ./init-flink.sh:/exec/init-flink.sh
      - ../../datagen/generated-v2:/build/datagen/generated

  flink-taskmanager:
    image: flink:1.16.1-scala_2.12-java11
    depends_on:
      - flink-jobmanager
    command: /bin/bash /exec/init-flink.sh taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 1
    volumes:
      - ./init-flink.sh:/exec/init-flink.sh
      - ../../datagen/generated-v2:/build/datagen/generated

  kafka:
    command:
      - redpanda
      - start
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:9094
      # Address the broker advertises to clients that connect to the Kafka API.
      # Use the internal addresses to connect to the Redpanda brokers'
      # from inside the same Docker network.
      # Use the external addresses to connect to the Redpanda brokers'
      # from outside the Docker network.
      - --advertise-kafka-addr internal://kafka:9092,external://localhost:9094
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      # Address the broker advertises to clients that connect to the HTTP Proxy.
      - --advertise-pandaproxy-addr internal://kafka:8082,external://localhost:18082
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      # Redpanda brokers use the RPC API to communicate with each other internally.
      - --rpc-addr kafka:33145
      - --advertise-rpc-addr kafka:33145
      # Mode dev-container uses well-known configuration properties for development in containers.
      - --mode dev-container
      # enable logs for debugging.
      - --default-log-level=info
    image: docker.redpanda.com/redpandadata/redpanda:v23.3.3
    container_name: kafka
    ports:
      - 18081:18081
      - 18082:18082
      - 9094:9094
      - 19644:9644

  kafka-setup:
    image: docker.io/bitnami/kafka:3.4.0-debian-11-r38
    volumes:
      - './create-topics.sh:/create-topics.sh'
    command: ['/bin/bash', '/create-topics.sh']
    depends_on:
      - kafka

  redpanda-ui:
    image: docker.redpanda.com/redpandadata/console:latest
    ports:
      - 8080:8080
    environment:
      KAFKA_BROKERS: kafka:9092
    depends_on:
      - kafka

  server:
    image: eclipse-temurin:11
    command: java -jar vertx-server.jar
    depends_on:
      - database
      - kafka-setup
    ports:
      - "8888:8888"
    volumes:
      - ./server-model.json:/server-model.json
      - ./server-config.json:/server-config.json
      - ./vertx-server.jar:/vertx-server.jar

  flink-job-submitter:
    image: badouralix/curl-jq:alpine
    depends_on:
      - flink-jobmanager
      - database
      - kafka-setup
    volumes:
      - ./flink-job.jar:/flink-job.jar
      - ./submit-flink-job.sh:/submit-flink-job.sh
    entrypoint: /submit-flink-job.sh

